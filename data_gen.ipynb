{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    from pyvi.ViTokenizer import tokenize\n",
    "    def strip_html_tags(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        stripped_text = soup.get_text(separator=\" \")\n",
    "        return stripped_text\n",
    "    text = strip_html_tags(text)\n",
    "    # remove 'BULLET::::-'\n",
    "    text = re.sub(r'BULLET::::-', ' ', text)\n",
    "    # remove = if more than 1\n",
    "    text = re.sub(r'={2,}', ' ', text)\n",
    "    # emove duplicate spaces\n",
    "    text = re.sub(r'  +', ' ', text)\n",
    "    # remove duplicate newline characters if more than 2\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    # remove all \\n if at the end of the text\n",
    "    text = re.sub(r'\\n+$', '', text)\n",
    "    # remove all \\n if at the beginning of the text\n",
    "    text = re.sub(r'^\\n+', '', text)\n",
    "\n",
    "    tokenize_text = tokenize(text)\n",
    "    return tokenize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('C:/Users/ngoph/Desktop/zalo_ai/wikipedia_20220620_cleaned/wikipedia_20220620_cleaned.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>revid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2577327</td>\n",
       "      <td>https://vi.wikipedia.org/wiki?curid=2577327</td>\n",
       "      <td>Amplicincia lathyi</td>\n",
       "      <td>Amplicincia lathyi\\n\\nAmplicincia lathyi là mộ...</td>\n",
       "      <td>2021-12-21 04:35:33+00:00</td>\n",
       "      <td>67703402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1912208</td>\n",
       "      <td>https://vi.wikipedia.org/wiki?curid=1912208</td>\n",
       "      <td>Kniphofia thomsonii</td>\n",
       "      <td>Kniphofia thomsonii\\n\\nKniphofia thomsonii là ...</td>\n",
       "      <td>2021-10-17 21:19:56+00:00</td>\n",
       "      <td>66396059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882686</td>\n",
       "      <td>https://vi.wikipedia.org/wiki?curid=1882686</td>\n",
       "      <td>Catasetum ollare</td>\n",
       "      <td>Catasetum ollare\\n\\nCatasetum ollare là một lo...</td>\n",
       "      <td>2021-12-20 00:59:12+00:00</td>\n",
       "      <td>67645152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1318025</td>\n",
       "      <td>https://vi.wikipedia.org/wiki?curid=1318025</td>\n",
       "      <td>Ciclova Română</td>\n",
       "      <td>Ciclova Română\\n\\nCiclova Română là một xã thu...</td>\n",
       "      <td>2022-02-07 11:43:35+00:00</td>\n",
       "      <td>68116159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>887188</td>\n",
       "      <td>https://vi.wikipedia.org/wiki?curid=887188</td>\n",
       "      <td>Euryglossa cupreochalybea</td>\n",
       "      <td>Euryglossa cupreochalybea\\n\\nEuryglossa cupreo...</td>\n",
       "      <td>2021-11-04 12:53:09+00:00</td>\n",
       "      <td>66679020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          url  \\\n",
       "0  2577327  https://vi.wikipedia.org/wiki?curid=2577327   \n",
       "1  1912208  https://vi.wikipedia.org/wiki?curid=1912208   \n",
       "2  1882686  https://vi.wikipedia.org/wiki?curid=1882686   \n",
       "3  1318025  https://vi.wikipedia.org/wiki?curid=1318025   \n",
       "4   887188   https://vi.wikipedia.org/wiki?curid=887188   \n",
       "\n",
       "                       title  \\\n",
       "0         Amplicincia lathyi   \n",
       "1        Kniphofia thomsonii   \n",
       "2           Catasetum ollare   \n",
       "3             Ciclova Română   \n",
       "4  Euryglossa cupreochalybea   \n",
       "\n",
       "                                                text  \\\n",
       "0  Amplicincia lathyi\\n\\nAmplicincia lathyi là mộ...   \n",
       "1  Kniphofia thomsonii\\n\\nKniphofia thomsonii là ...   \n",
       "2  Catasetum ollare\\n\\nCatasetum ollare là một lo...   \n",
       "3  Ciclova Română\\n\\nCiclova Română là một xã thu...   \n",
       "4  Euryglossa cupreochalybea\\n\\nEuryglossa cupreo...   \n",
       "\n",
       "                  timestamp     revid  \n",
       "0 2021-12-21 04:35:33+00:00  67703402  \n",
       "1 2021-10-17 21:19:56+00:00  66396059  \n",
       "2 2021-12-20 00:59:12+00:00  67645152  \n",
       "3 2022-02-07 11:43:35+00:00  68116159  \n",
       "4 2021-11-04 12:53:09+00:00  66679020  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b608db0236f4287aa0fe0998e570b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=212245), Label(value='0 / 212245')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df['text'].parallel_apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Bệnh viện Grall\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bệnh viện Grall\\nBệnh viện Grall là một bệnh viện lớn ở Sài Gòn hoạt động từ năm 1925 đến 1978.\\n Lịch sử. \\nTiền thân của Bệnh viện Grall là Bệnh viện Quân sự (tiếng Pháp: \"Hôpital militaire\") của Quân đội Pháp, thành lập từ năm 1862 khi họ mới xâm chiếm Nam Kỳ. Cơ sở này vào cuối thập niên 1870 chuyển về số 14 rue Lagrandière, tức địa điểm hiện nay. Tại cơ sở này nhà bác học Albert Calmette cho thành lập Viện Pasteur (Pasteur-Institut) đầu tiên ở ngoài nước Pháp năm 1891.\\nCấu trúc các tòa nhà trong khuôn viên đều là sườn sắt tiền chế đem ráp lại trên nền bằng đá. Mọi vật liệu mang từ Pháp sang.\\nTừ năm 1905 trở đi cơ sở y tế này dưới sự điều hành của bác sĩ Charles Grall, mở cửa chữa trị cho mọi thành phần, quân sự cũng như dân sự kể cả dân bản xứ. Năm 1925 Bệnh viện Quân sự chính thức sang tên \"Bệnh viện Grall\" để vinh danh Giám đốc Y tế Nam Kỳ, bác sĩ Charles Grall. Tháng Tư năm 1945 thời Đệ nhị Thế chiến bệnh viện bị trúng bom, phá sập mé phía bắc, tiêu hủy các phòng thí nghiệm. \\nNăm 1956 dưới chính thể Việt Nam Cộng hòa, chính phủ Pháp ký biên bản tiếp tục điều hành Bệnh viện Grall, thuộc Bộ Ngoại giao Pháp. Bệnh viện có 560 giường.\\nNgày 3 Tháng 11, 1966 Bệnh viện trúng pháo của Việt Cộng. Vào cuối Tháng Tư, 1975 trong đợt tấn công cuối cùng vào Sài Gòn, Bệnh viện Grall bị tràn ngập, bệnh nhân trọng thương vì chiến trận lên đến 222 người chỉ trong ba ngày cuối cùng.\\nNăm 1976 Bệnh viện Grall chuyển giao cho nhà chức trách Cộng hòa Xã hội Chủ nghĩa Việt Nam và người Pháp rút đi. \\nNăm 1978 Bệnh viện Grall đổi tên thành Bệnh viện Nhi đồng 2, chấm dứt thời kỳ bệnh viện tổng quát và trở thành bệnh viện chuyên môn nhi khoa.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam = df[df['title']=='Bệnh viện Grall']\n",
    "print('title:',exam.title.values[0])\n",
    "exam.text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_json('data_gen/wikipedia_20220620_cleaned_parsed.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\ngoph/.cache\\torch\\sentence_transformers\\VoVanPhuc_sup-SimCSE-VietNamese-phobert-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\ngoph/.cache\\torch\\sentence_transformers\\VoVanPhuc_sup-SimCSE-VietNamese-phobert-base were not used when initializing RobertaModel: ['mlp.dense.weight', 'mlp.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "model = SentenceTransformer('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base',device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data_gen/wikipedia_20220620_cleaned_parsed.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c25aac1d12e47eb94a51291c4cc7d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=212245), Label(value='0 / 212245')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Nhơn_Đức',\n",
       " 'Dobin am See',\n",
       " 'Hyloniscus_rilensis',\n",
       " 'Chesias_ornata',\n",
       " 'Membrey',\n",
       " 'Bộ Nông_nghiệp và Phát_triển nông_thôn ( Việt_Nam )',\n",
       " 'Thành cổ Baku',\n",
       " 'Digitaria_effusa',\n",
       " 'Cyclodictyon_immersum',\n",
       " 'Psychotria_nervosa',\n",
       " 'Nodulotrophon_septus',\n",
       " 'Griphapex_scutellaris',\n",
       " 'Le_Mesnil - Amelot',\n",
       " 'Bảng tổng_sắp huy_chương Thế vận_hội Mùa đông 2018',\n",
       " 'Kauai',\n",
       " 'Tirsa_fiona',\n",
       " 'Teinobasis_filamenta',\n",
       " 'Nippon_Steel',\n",
       " 'Eublemma_minutata',\n",
       " 'Valeri_Kaptilov']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = df['title'].parallel_apply(tokenize).tolist()\n",
    "titles[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(titles, open('data_gen/titles.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_embeddings = model.encode(titles,convert_to_tensor=True,show_progress_bar=True,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(title_embeddings, open('data_gen/title_embeddings.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = pickle.load(open('data_gen/title_embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huyện Tuy_An\n",
      "Tuy_An - 0.9403116106987\n",
      "An_Thọ , Tuy_An - 0.8956291675567627\n",
      "An_Hiệp , Tuy_An - 0.8954504728317261\n",
      "An_Cư , Tuy_An - 0.8625949621200562\n",
      "An_Ninh_Đông , Tuy_An - 0.8604764938354492\n",
      "An_Định , Tuy_An - 0.8470880389213562\n",
      "An_Ninh_Tây , Tuy_An - 0.8405049443244934\n",
      "An_Mỹ , Tuy_An - 0.8361471891403198\n",
      "Phú_Yên ( xã ) - 0.7179363965988159\n",
      "Phú_Yên - 0.7162452936172485\n"
     ]
    }
   ],
   "source": [
    "q_raw = '''huyện Tuy An'''\n",
    "q = tokenize(q_raw)\n",
    "query_embedding = model.encode(q,convert_to_tensor=True,)\n",
    "print(q)\n",
    "hits = util.semantic_search(query_embedding, title_embeddings, top_k=10)[0]\n",
    "\n",
    "answers =[]\n",
    "for hit in hits:\n",
    "    corpus_id = hit['corpus_id']\n",
    "    doc_score = hit['score']\n",
    "    title = titles[corpus_id]\n",
    "    print(f'{title} - {doc_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sentences(row):\n",
    "    from pyvi.ViTokenizer import tokenize\n",
    "    title=row['title']\n",
    "    text=row['text']\n",
    "    return [tokenize(title),text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1273469"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(sentences,open('data_gen/sentences.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pickle.load(open('data_gen/sentences.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode(sentences,convert_to_tensor=True,show_progress_bar=True,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save corpus_embeddings\n",
    "# pickle.dump(corpus_embeddings,open('data_gen/corpus_embeddings_2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = pickle.load(open('data_gen/corpus_embeddings.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1273469, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at VoVanPhuc/sup-SimCSE-VietNamese-phobert-base were not used when initializing RobertaForSequenceClassification: ['mlp.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'mlp.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at VoVanPhuc/sup-SimCSE-VietNamese-phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "cross_encoder = CrossEncoder('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/zac2022_train_merged_final.json',encoding='utf-8') as f:\n",
    "    data=json.load(f)\n",
    "    ddf = pd.json_normalize(data,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf=ddf[ddf['title']!='']\n",
    "ddf=ddf[ddf['category']=='FULL_ANNOTATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ddf.sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huyện Tịnh Biên của tỉnh an giang có rừng tràm nổi tiếng nào\n",
      "An Giang\n",
      "wiki/Rừng_tràm_Trà_Sư\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.787\tTịnh_Biên\n",
      "\t0.767\tRừng tràm Trà_Sư\n",
      "\t0.766\tCửa_khẩu Tịnh_Biên\n",
      "\t0.748\tTịnh_Biên ( thị_trấn )\n",
      "\t0.748\tKhu kinh_tế cửa_khẩu An_Giang\n",
      "\t0.730\tThới_Sơn ( định_hướng )\n",
      "\t0.719\tAn Nông , Tịnh_Biên\n",
      "\t0.714\tAn_Cư , Tịnh_Biên\n",
      "\t0.710\tNúi_Dài Năm_Giếng\n",
      "\t0.708\tAn_Phú , Tịnh_Biên\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t0.500\tNúi_Nước\n",
      "\t0.499\tChùa Thới_Sơn\n",
      "\t0.496\tCleistanthus_travancorensis\n",
      "\t0.496\tBảy_Núi\n",
      "\t0.494\tChà_chôi\n",
      "\t0.494\tLương_Phi\n",
      "\t0.494\tCleistanthus_namatanaiensis\n",
      "\t0.493\tNúi_Sam\n",
      "\t0.492\tChaunus_gallardoi\n",
      "\t0.490\tKhóm Tắc_Cậu\n"
     ]
    }
   ],
   "source": [
    "q_raw = example['question']\n",
    "q = tokenize(q_raw)\n",
    "query_embedding = model.encode(q,convert_to_tensor=True,)\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=100)[0]\n",
    "\n",
    "##### Re-Ranking #####\n",
    "# Now, score all retrieved passages with the cross_encoder\n",
    "cross_inp = [[q, sentences[hit['corpus_id']][1]] for hit in hits]\n",
    "cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "# Sort results by the cross-encoder scores\n",
    "for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx]\n",
    "# Output of top-5 hits from bi-encoder\n",
    "print(q_raw)\n",
    "print(example.title)\n",
    "print(example.answer)\n",
    "print(\"\\n-------------------------\\n\")\n",
    "print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "for hit in hits[0:10]:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], sentences[hit['corpus_id']][0]))\n",
    "\n",
    "# Output of top-5 hits from re-ranker\n",
    "print(\"\\n-------------------------\\n\")\n",
    "print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "for hit in hits[0:10]:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], sentences[hit['corpus_id']][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba141c8d811672bca916348b96b465f0d8c36918c76b97082af4ef2ad5ae2a45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
